{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFCLayers1(model):\n",
    "    output = GlobalAveragePooling2D()(model.output)\n",
    "    output = Dense(512, activation='relu')(output)\n",
    "    pred = Dense(12, activation='softmax')(output)\n",
    "\n",
    "    # Create graph of new model\n",
    "    return Model(input=model.input, output=pred)\n",
    "\n",
    "\n",
    "def addFCLayers2(model):\n",
    "    output = Flatten()(model.output)\n",
    "    output = Dense(1024, activation=\"relu\")(output)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = Dense(512, activation=\"relu\")(output)\n",
    "    pred = Dense(12, activation=\"softmax\")(output)\n",
    "\n",
    "    # Create graph of new model\n",
    "    return Model(input=model.input, output=pred)\n",
    "\n",
    "\n",
    "def addFCLayers3(model):\n",
    "    output = Flatten()(model.output)\n",
    "    output = Dense(512, activation='relu')(output)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    pred = Dense(12, activation='softmax')(output)\n",
    "\n",
    "    # Create graph of new model\n",
    "    return Model(input=model.input, output=pred)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\" \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    if normalize == False:\n",
    "        plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_confusion_matrix(model):\n",
    "    \"\"\" 'model' should be the final trained model \"\"\"\n",
    "\n",
    "    # load test data\n",
    "    test_gen = ImageDataGenerator().flow_from_directory(\n",
    "        './data/testing/',\n",
    "        target_size=(299, 299),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "    # predict test data\n",
    "    pred_labels = model4.predict_generator(test_gen)\n",
    "    y_pred = np.argmax(pred_labels, axis=1)  # get highest prob class\n",
    "    true_labels = test_gen.classes  # get true labels\n",
    "\n",
    "    # list of class names\n",
    "    classNames = sorted(set([cls.split('/')[0] for cls in test_gen.filenames]))\n",
    "\n",
    "    # convert index of classes to class names\n",
    "    le = LabelEncoder()\n",
    "    le.fit(classNames)\n",
    "    y_labels = le.inverse_transform(test_gen.classes)\n",
    "    y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "    return confusion_matrix(y_labels, y_pred), classNames\n",
    "\n",
    "\n",
    "def plotMetrics(data, title):\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(data['acc'])\n",
    "    plt.plot(data['val_acc'])\n",
    "    plt.title('Model Accuracy ({})'.format(title))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(data['loss'])\n",
    "    plt.plot(data['val_loss'])\n",
    "    plt.title('Model Loss ({})'.format(title))\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "    print(\"Final training acc: {}, loss: {}\".format(data['acc'][-1], data['loss'][-1]))\n",
    "    print(\"Final validation acc: {}, loss: {}\".format(data['val_acc'][-1], data['val_loss'][-1]))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def bestOptimizer(model,\n",
    "                  title,\n",
    "                  epochs,\n",
    "                  batch_size,\n",
    "                  n_train_samples,\n",
    "                  n_validation_samples,\n",
    "                  callback,\n",
    "                  train_gen,\n",
    "                  validation_gen,\n",
    "                  test_gen):\n",
    "    ########## ADAM ##########\n",
    "    model = addFCLayers3(model)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "                  metrics=['accuracy'])\n",
    "    hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=n_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_gen,\n",
    "        callbacks=[callback],\n",
    "        validation_steps=n_validation_samples // batch_size)\n",
    "    test_data = model.evaluate_generator(test_gen)\n",
    "    hist.history['test'] = test_data\n",
    "    model.save(title + '_adam.h5')\n",
    "    with open(title + '_adam_hist.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(hist.history, file_pi)\n",
    "\n",
    "    ########## SGD ##########\n",
    "    model = addFCLayers3(model)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=True),\n",
    "                  metrics=['accuracy'])\n",
    "    hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=n_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_gen,\n",
    "        callbacks=[callback],\n",
    "        validation_steps=n_validation_samples // batch_size)\n",
    "    test_data = model.evaluate_generator(test_gen)\n",
    "    hist.history['test'] = test_data\n",
    "    model.save(title + '_SGD.h5')\n",
    "    with open(title + '_SGD_hist.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(hist.history, file_pi)\n",
    "\n",
    "    ########## RMSPROP ##########\n",
    "    model = addFCLayers3(model)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "    hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=n_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_gen,\n",
    "        callbacks=[callback],\n",
    "        validation_steps=n_validation_samples // batch_size)\n",
    "    test_data = model.evaluate_generator(test_gen)\n",
    "    hist.history['test'] = test_data\n",
    "    model.save(title + '_rmsprop.h5')\n",
    "    with open(title + '_rmsprop_hist.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(hist.history, file_pi)\n",
    "\n",
    "\n",
    "def bestLR(model,\n",
    "           title,\n",
    "           lrRange,\n",
    "           optim,\n",
    "           epochs,\n",
    "           batch_size,\n",
    "           n_train_samples,\n",
    "           n_validation_samples,\n",
    "           callback,\n",
    "           train_gen,\n",
    "           validation_gen,\n",
    "           test_gen):\n",
    "    for lr in lrRange:\n",
    "        print(\"---> Learning rate: {}\".format(lr))\n",
    "        model = addFCLayers3(model)\n",
    "        # choose correct optimizer\n",
    "        if optim == 'Adam':\n",
    "            opt = optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "        elif optim == 'SGD':\n",
    "            opt = optimizers.SGD(lr=lr, momentum=0.0, decay=0.0, nesterov=True)\n",
    "        elif optim == 'RMSprop':\n",
    "            opt = optimizers.RMSprop(lr=lr, rho=0.9, epsilon=None, decay=0.0)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "        hist = model.fit_generator(\n",
    "            train_gen,\n",
    "            steps_per_epoch=n_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_gen,\n",
    "            callbacks=[callback],\n",
    "            validation_steps=n_validation_samples // batch_size)\n",
    "        test_data = model.evaluate_generator(test_gen)\n",
    "        hist.history['test'] = test_data\n",
    "        model.save(title + '_lr_{}.h5'.format(lr))\n",
    "        with open(title + '_lr_{}_hist.pkl'.format(lr), 'wb') as file_pi:\n",
    "            pickle.dump(hist.history, file_pi)\n",
    "\n",
    "\n",
    "def bestFCLayer(model,\n",
    "                title,\n",
    "                lr,\n",
    "                optim,\n",
    "                epochs,\n",
    "                batch_size,\n",
    "                n_train_samples,\n",
    "                n_validation_samples,\n",
    "                callback,\n",
    "                train_gen,\n",
    "                validation_gen,\n",
    "                test_gen):\n",
    "    # choose correct optimizer\n",
    "    if optim == 'Adam':\n",
    "        opt = optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    elif optim == 'SGD':\n",
    "        opt = optimizers.SGD(lr=lr, momentum=0.0, decay=0.0, nesterov=True)\n",
    "    elif optim == 'RMSprop':\n",
    "        opt = optimizers.RMSprop(lr=lr, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "    ########## FC1 ##########\n",
    "    model = addFCLayers1(model)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=n_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_gen,\n",
    "        callbacks=[callback],\n",
    "        validation_steps=n_validation_samples // batch_size)\n",
    "    test_data = model.evaluate_generator(test_gen)\n",
    "    hist.history['test'] = test_data\n",
    "    model.save(title + '_fc1.h5')\n",
    "    with open(title + '_fc1_hist.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(hist.history, file_pi)\n",
    "\n",
    "    ########## FC2 ##########\n",
    "    model = addFCLayers2(model)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=n_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_gen,\n",
    "        callbacks=[callback],\n",
    "        validation_steps=n_validation_samples // batch_size)\n",
    "    test_data = model.evaluate_generator(test_gen)\n",
    "    hist.history['test'] = test_data\n",
    "    model.save(title + '_fc2.h5')\n",
    "    with open(title + '_fc2_hist.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(hist.history, file_pi)\n",
    "\n",
    "    ########## FC3 ##########\n",
    "    model = addFCLayers3(model)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=n_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_gen,\n",
    "        callbacks=[callback],\n",
    "        validation_steps=n_validation_samples // batch_size)\n",
    "    test_data = model.evaluate_generator(test_gen)\n",
    "    hist.history['test'] = test_data\n",
    "    model.save(title + '_fc3.h5')\n",
    "    with open(title + '_fc3_hist.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(hist.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "\n",
    "##################################################################\n",
    "#                       SETUP DATA GENERATORS\n",
    "##################################################################\n",
    "print(\"Setting up data generators...\")\n",
    "# init real-time data augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    './data/training',\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "validation_gen = ImageDataGenerator().flow_from_directory(\n",
    "    './data/validation',\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "test_gen = ImageDataGenerator().flow_from_directory(\n",
    "    './data/testing/',\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Init early stopping functionality\n",
    "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "##################################################################\n",
    "#                       SETUP MODELS\n",
    "##################################################################\n",
    "print(\"Setting up models...\")\n",
    "# VGG16\n",
    "vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(299, 299, 3), pooling=None)\n",
    "for layer in vgg16.layers:  # do not train\n",
    "    layer.trainable = False\n",
    "\n",
    "# VGG19\n",
    "vgg19 = VGG19(include_top=False, weights='imagenet', input_tensor=None, input_shape=(299, 299, 3), pooling=None)\n",
    "for layer in vgg19.layers:  # do not train\n",
    "    layer.trainable = False\n",
    "\n",
    "# # Xception\n",
    "xception = Xception(include_top=False, input_tensor=None, input_shape=(299, 299, 3), pooling='None')\n",
    "for layer in xception.layers:  # do not train\n",
    "    layer.trainable = False\n",
    "\n",
    "##################################################################\n",
    "#                       BEST OPTIMIZER\n",
    "##################################################################\n",
    "print(\"------ Best Optimizer ------\")\n",
    "\n",
    "n_train_samples = 12210\n",
    "n_validation_samples = 2572\n",
    "\n",
    "### VGG16\n",
    "ut.bestOptimizer(vgg16, 'vgg16', epochs, batch_size, n_train_samples, n_validation_samples, callback, train_gen,\n",
    "                 validation_gen, test_gen)\n",
    "\n",
    "### VGG19\n",
    "ut.bestOptimizer(vgg19, 'vgg19', epochs, batch_size, n_train_samples, n_validation_samples, callback, train_gen,\n",
    "                 validation_gen, test_gen)\n",
    "\n",
    "### VGG19\n",
    "ut.bestOptimizer(xception, 'xception', epochs, batch_size, n_train_samples, n_validation_samples, callback, train_gen,\n",
    "                 validation_gen, test_gen)\n",
    "\n",
    "##################################################################\n",
    "#                       BEST LEARNING RATE\n",
    "##################################################################\n",
    "print(\"------ Best learning rate ------\")\n",
    "\n",
    "lrRange = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "### VGG16\n",
    "ut.bestLR(vgg16, 'vgg16', lrRange, 'SGD', epochs, batch_size, n_train_samples, n_validation_samples, callback,\n",
    "          train_gen, validation_gen, test_gen)\n",
    "\n",
    "### VGG19\n",
    "ut.bestLR(vgg19, 'vgg19', lrRange, 'Adam', epochs, batch_size, n_train_samples, n_validation_samples, callback,\n",
    "          train_gen, validation_gen, test_gen)\n",
    "\n",
    "### VGG19\n",
    "ut.bestLR(xception, 'xception', lrRange, 'SGD', epochs, batch_size, n_train_samples, n_validation_samples, callback,\n",
    "          train_gen, validation_gen, test_gen)\n",
    "\n",
    "##################################################################\n",
    "#                       BEST CLASSIFICATION LAYER\n",
    "##################################################################\n",
    "print(\"------ Best classification layer ------\")\n",
    "\n",
    "### VGG16\n",
    "ut.bestFCLayer(vgg16, 'vgg16', .01, 'SGD', epochs, batch_size, n_train_samples, n_validation_samples, callback,\n",
    "               train_gen, validation_gen, test_gen)\n",
    "\n",
    "### VGG19\n",
    "ut.bestFCLayer(vgg19, 'vgg19', .001, 'Adam', epochs, batch_size, n_train_samples, n_validation_samples, callback,\n",
    "               train_gen, validation_gen, test_gen)\n",
    "\n",
    "### VGG19\n",
    "ut.bestFCLayer(xception, 'xception', .01, 'SGD', epochs, batch_size, n_train_samples, n_validation_samples, callback,\n",
    "               train_gen, validation_gen, test_gen)\n",
    "\n",
    "##################################################################\n",
    "#                       PLOT CONFUSION MATRIX\n",
    "##################################################################\n",
    "print(\"------ Plot confusion matrix ------\")\n",
    "\n",
    "### Load your model first! \n",
    "model = addFCLayers3(xception)\n",
    "model.load_weights('xception_fc3.h5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=.01, momentum=0.0, decay=0.0, nesterov=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cm, classNames = ut.get_confusion_matrix(model)\n",
    "ut.plot_confusion_matrix(cm, classNames, normalize=True, title='Confusion Matrix for Xception')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
